{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Combined - LSTM for (merged) text + Dense for others\n",
    "\n",
    "Our NN models vary in the way we preprocess the text features (most important feature) towards feeding it into our NN.\n",
    "This also effects the architechture of the NN.\n",
    "\n",
    "In this notebook we:\n",
    "1. Merge all text features (Title, Description, params) into one feature.\n",
    "2. We learn the representation of the text feature by an LSTM (its output vector is the text representation).\n",
    "3. We combine the LSTMed text with all other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glob/intel-python/versions/2018/intelpython3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning:\n",
      "\n",
      "Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run stephan_modules.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/u14303/Avito'\n",
    "HELPER_DATA_PATH = '/home/u14303/Avito/helper_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n"
     ]
    }
   ],
   "source": [
    "print('loading data...')\n",
    "train, test = load_data(DATA_PATH)\n",
    "train, test = basic_enrichment(train, test, helper_data_path=HELPER_DATA_PATH)\n",
    "train, test = load_image_features(train, test, helper_data_path=HELPER_DATA_PATH)\n",
    "train, test = load_text_features(train, test, helper_data_path=HELPER_DATA_PATH)\n",
    "train, test = add_aggregated_features(train, test, helper_data_path=HELPER_DATA_PATH)\n",
    "train, test = numeric_features_cleaning(train, test, helper_data_path=HELPER_DATA_PATH)\n",
    "train, test = complete_image_top_1(train, test, helper_data_path=HELPER_DATA_PATH)\n",
    "train, test = complete_price(train, test, helper_data_path=HELPER_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize features towards input to an NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_features = ['title', 'description', 'param123']\n",
    "text_feature = 'title_description_params'\n",
    "cat_features = ['user_type', \\\n",
    "                'region', 'city', \\\n",
    "                'parent_category_name', 'category_name', 'param_1', 'param_2', 'param_3', \\\n",
    "                'image_top_1_class', 'image_top_1_rounded_regression', \\\n",
    "                'month', 'day', 'weekday', \\\n",
    "                'has_price', 'has_description', 'has_params', 'has_image']\n",
    "cont_ord_features = ['image_top_1_regression', \\\n",
    "                     'log_price_regression', \\\n",
    "                     'avg_days_up_user', 'avg_times_up_user', 'n_user_items', 'user_ads_count', \\\n",
    "                     'log_item_seq_number', \\\n",
    "                     'img_size', 'img_luminance', 'img_colorfulness', 'img_confidence', 'log_img_sharpness', 'log_img_keypoints', \\\n",
    "                     'title_word_count', 'description_word_count', 'merged_params_word_count', \\\n",
    "                     'description_non_regular_chars_ratio', 'title_capital_letters_ratio','description_capital_letters_ratio', \\\n",
    "                     'title_non_regular_chars_ratio', 'title_adj_to_len_ratio', 'title_noun_to_len_ratio',\\\n",
    "                     'title_sentiment']\n",
    "\n",
    "train_y_prob = train['deal_probability']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text towards input to an LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "stopwords = set()\n",
    "with codecs.open(('stopwords_ru.txt'), encoding='cp1251') as ins:\n",
    "    for w in ins:\n",
    "        word = w.strip(\"\\r\\n\")\n",
    "        word = word.strip(\"\\n\")\n",
    "        stopwords.add(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenize and Vectorize (keras encoded one-hot representation (each onehot vec represented as an int number)) text feature.\n",
    "\n",
    "See: https://keras.io/preprocessing/text/#one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot, Tokenizer, text_to_word_sequence\n",
    "\n",
    "# Those consts are important for the NN itself\n",
    "max_words_in_desc_title_param = 150 # See text analysis notebook. 95% are shorter than a 110.\n",
    "word_embed_dim = 300\n",
    "\n",
    "train_x_text = train[[text_feature]]\n",
    "test_x_text = test[[text_feature]]\n",
    "train_x_text[text_feature] = train_x_text[text_feature].str.lower()\n",
    "test_x_text[text_feature] = test_x_text[text_feature].str.lower()\n",
    "tokenizer = Tokenizer(num_words = max_words_in_desc_title_param)\n",
    "all_text = np.hstack([train_x_text[text_feature], test_x_text[text_feature]])\n",
    "\n",
    "print('Tokenizing text...')\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "print('Done tokenizing.')\n",
    "\n",
    "del all_text\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# By default text_to_word_sequence automatically does 4 things:\n",
    "#   Splits words by space (split=” “), \n",
    "#   Filters out punctuation (filters=’!”#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n’).\n",
    "#   Converts text to lowercase (lower=True).\n",
    "# We add stopwords filtering to the process.\n",
    "\n",
    "def my_text_to_word_sequence(text):\n",
    "    result = []\n",
    "    for word in text_to_word_sequence(text):\n",
    "        if word not in stopwords:\n",
    "            result.append(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Applying tokenizer on text...')\n",
    "\n",
    "train_x_text[text_feature] = train_x_text.apply(lambda r: [tokenizer.word_index[word] for word in my_text_to_word_sequence(r[text_feature])], axis=1)\n",
    "test_x_text[text_feature] = test_x_text.apply(lambda r: [tokenizer.word_index[word] for word in my_text_to_word_sequence(r[text_feature])], axis=1)\n",
    "    \n",
    "train_x_text = pad_sequences(train_x_text[text_feature], maxlen=max_words_in_desc_title_param)\n",
    "test_x_text = pad_sequences(test_x_text[text_feature], maxlen=max_words_in_desc_title_param)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and load FastText (Facebook's) Russian wikipedia word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasttext_embedding_matrix(data_path, tokenizer, embedding_dim):\n",
    "    print('loading embeddings...')\n",
    "    \n",
    "    EMBEDDING_FILE_PATH = os.path.join(data_path, 'cc.ru.300.vec')\n",
    "    def get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE_PATH))\n",
    "\n",
    "    vocab_size = len(tokenizer.word_index) + 2\n",
    "    embedding_matrix = np.random.rand(vocab_size, embedding_dim)\n",
    "    \n",
    "    print('creating embedding matrix...')\n",
    "    embedding_exists = 0\n",
    "    no_embeddings = 0\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "            embedding_exists += 1\n",
    "        else:\n",
    "            no_embeddings += 1\n",
    "    \n",
    "    print (\"There are total of {} words in our corpus.\".format(embedding_exists+no_embeddings))\n",
    "    print (\"There are {} embeddings in FastText.\".format(len(embeddings_index)))\n",
    "    print (\"We have embeddings for {} words ({}% existing embeddings).\".format(embedding_exists, \\\n",
    "                                                                               (100*embedding_exists/(embedding_exists+no_embeddings))))\n",
    "    print (\"Embedding is missing for {} words.\".format(no_embeddings))\n",
    "\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    \n",
    "    print('done loading embeddings...')\n",
    "    return embedding_matrix, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix, vocab_size = get_fasttext_embedding_matrix(data_path=DATA_PATH, \\\n",
    "                                                             tokenizer=tokenizer, embedding_dim = word_embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical features.\n",
    "\n",
    "Vectorize all loaded categorical features.\n",
    "\n",
    "\n",
    "See: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x_cat = train[cat_features]\n",
    "test_x_cat = test[cat_features]\n",
    "for col in cat_features:\n",
    "    train_x_cat[col] = train_x_cat[col].astype('category')\n",
    "    test_x_cat[col] = test_x_cat[col].astype('category')\n",
    "\n",
    "# Encode to integers.\n",
    "# For vectorization (encoding) we concat both train and test into one\n",
    "all_cat = pd.concat([train_x_cat, test_x_cat], axis = 0)\n",
    "for col in cat_features:\n",
    "    enc = preprocessing.LabelEncoder().fit(all_cat[col])\n",
    "    train_x_cat[col] = enc.transform(train_x_cat[col])\n",
    "    test_x_cat[col] = enc.transform(test_x_cat[col])\n",
    "\n",
    "# One-hot encode:\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(pd.concat([train_x_cat, test_x_cat], axis = 0))\n",
    "train_x_cat = enc.transform(train_x_cat)\n",
    "test_x_cat = enc.transform(test_x_cat)\n",
    "\n",
    "del all_cat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical(Continious/Ordinal) features\n",
    "\n",
    "Normalize all loaded numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_numerical = train[cont_ord_features]\n",
    "test_x_numerical = test[cont_ord_features]\n",
    "train_x_numerical.fillna(0, inplace = True)\n",
    "test_x_numerical.fillna(0, inplace = True)\n",
    "for col in cont_ord_features:\n",
    "    train_x_numerical[col] = train_x_numerical[col].astype('float64')\n",
    "    test_x_numerical[col] = test_x_numerical[col].astype('float64')\n",
    "\n",
    "# Normalize features:\n",
    "train_x_numerical = normalize(train_x_numerical, axis=0)\n",
    "test_x_numerical = normalize(test_x_numerical, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning - Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we should have a pad sequence out of the text features\n",
    "All categorials should be onehot encoded\n",
    "All continious/ordinal should be vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO(): Try hidden_dim1 = 512, hidden_dim2 = 128 after loading FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_out_dim = 256\n",
    "cat_hidden_dim = 128\n",
    "merged_hidden_dim = 256\n",
    "out_dim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that gets a tokenizer, and returns fasttext_embedding_matrix that will be loaded into the embedding layer of our NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "l_text_input = Input(shape=(max_words_in_desc_title_param,), name=\"text_lstm_input\")\n",
    "# Load FastText's weights:\n",
    "l_text_embedding = Embedding(input_dim=vocab_size, output_dim=word_embed_dim, weights = [embedding_matrix], \\\n",
    "                             input_length=max_words_in_desc_title_param, name='text_embeddings')(l_text_input)\n",
    "lstm_out = LSTM(lstm_out_dim, name='LSTM', dropout=0.2, recurrent_dropout=0.2)(l_text_embedding)\n",
    "\n",
    "# Categoricals\n",
    "l_cat_input = Input(shape=(train_x_cat.shape[1],), sparse=True, name=\"cat_input\")\n",
    "l_hidden_cat = Dense(cat_hidden_dim, activation='relu',\n",
    "                     kernel_regularizer=regularizers.l2(1e-6), name='l_hidden_cat')(l_cat_input)\n",
    "\n",
    "# Numerical\n",
    "l_numerical_input = Input(shape=(train_x_numerical.shape[1],), name=\"numerical_input\")\n",
    "\n",
    "# Aggregate all inputs into one hidden layer.\n",
    "l_aggregative = concatenate([l_hidden_text, l_hidden_cat, l_numerical_input])\n",
    "\n",
    "l_merged_hidden = Dense(merged_hidden_dim, activation='relu',\n",
    "                            kernel_regularizer=regularizers.l2(1e-6), name='l_merged_hidden')(l_aggregative)\n",
    "\n",
    "output = Dense(out_dim, activation='sigmoid', name='output')(l_merged_hidden) # This is the logistic regression output\n",
    "\n",
    "rmsprop_opt = RMSprop(lr=0.00001) # Best for training RNNs.\n",
    "def rmse_err(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "\n",
    "model = Model(inputs=[l_text_input, l_cat_input, l_numerical_input], outputs=main_output)\n",
    "model.compile(optimizer=rmsprop_opt,\n",
    "              ,loss=[rmse_err]) # Higher weight for main output.\n",
    "print(model.summary())\n",
    "\n",
    "load = False\n",
    "fname = 'NN-LSTM-COMBINED-MERGED-model-weights.h5'\n",
    "if load:\n",
    "    model.load_weights(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr_cd = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "save_weights_cd = ModelCheckpoint(fname, save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    " = 256\n",
    "\n",
    "# We want to double batch size every two epochs.\n",
    "nepochs = 4\n",
    "for i in range(nepochs):\n",
    "    print ('Curr batch size: {}'.format(nbatch_size))\n",
    "    model.fit(train_x_text, train_y_prob, \n",
    "              validation_split = 0.15, \n",
    "              epochs=2, \n",
    "              batch_size=nbatch_size,\n",
    "              callbacks=[reduce_lr_cd, save_weights_cd])\n",
    "    nbatch_size *= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions to futher ensambels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict({'text_lstm_input': train_x_text, 'cat_input': train_x_cat, 'numerical_input': train_x_numerical})\n",
    "y_pred_test = model.predict({'text_lstm_input': test_x_text, 'cat_input': test_x_cat, 'numerical_input': test_x_numerical})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train_df = pd.DataFrame(train['item_id'])\n",
    "res_train_df['deal_probability'] = y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_test_df = pd.DataFrame(test['item_id'])\n",
    "res_test_df['deal_probability'] = y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.concat([res_train_df, res_test_df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv('/home/u14303/Avito/Predictions/NN-LSTM-COMBINED-MERGED.csv.gz', index=False, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
