{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: NN - Just LSTM\n",
    "\n",
    "Our NN models vary in the way we preprocess the text features (most important feature) towards feeding it into our NN.\n",
    "This also effects the architechture of the NN.\n",
    "\n",
    "In this notebook we run seperate LSTMs on \"title\" and \"description\" for all ads. We learn \"y_probability\" (the target feature) directly from the LSTMs output vectors (hidden layers) *without* combining it with any other feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glob/intel-python/versions/2018/intelpython3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning:\n",
      "\n",
      "Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run stephan_modules.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/u14303/Avito'\n",
    "HELPER_DATA_PATH = '/home/u14303/Avito/helper_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "Adding basic features...\n",
      "Done adding basic features.\n",
      "Adding image features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u14303/stephan_feature_enrichment.py:65: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "/home/u14303/stephan_feature_enrichment.py:66: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading image features.\n",
      "Loading text features...\n",
      "Done loading text features.\n",
      "Loading aggregated features...\n",
      "Done loading aggregated features.\n",
      "Loading aggregated features...\n",
      "Done loading aggregated features.\n",
      "Cleaning and completing numeric features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u14303/stephan_feature_enrichment.py:172: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "/home/u14303/stephan_feature_enrichment.py:173: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done cleaning numeric features.\n",
      "Completing image_top_1 features...\n",
      "Done loading image_top_1 completions.\n",
      "Completing price...\n",
      "Done loading log_price_regression.\n"
     ]
    }
   ],
   "source": [
    "print('loading data...')\n",
    "train, test = load_data(DATA_PATH)\n",
    "train, test = basic_enrichment(train, test, helper_data_path=HELPER_DATA_PATH)\n",
    "train, test = load_image_features(train, test, helper_data_path=HELPER_DATA_PATH)\n",
    "train, test = load_text_features(train, test, helper_data_path=HELPER_DATA_PATH)\n",
    "train, test = add_aggregated_features(train, test, helper_data_path=HELPER_DATA_PATH)\n",
    "train, test = numeric_features_cleaning(train, test, helper_data_path=HELPER_DATA_PATH)\n",
    "train, test = complete_image_top_1(train, test, helper_data_path=HELPER_DATA_PATH)\n",
    "train, test = complete_price(train, test, helper_data_path=HELPER_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item_id',\n",
       " 'user_id',\n",
       " 'region',\n",
       " 'city',\n",
       " 'parent_category_name',\n",
       " 'category_name',\n",
       " 'param_1',\n",
       " 'param_2',\n",
       " 'param_3',\n",
       " 'title',\n",
       " 'description',\n",
       " 'price',\n",
       " 'item_seq_number',\n",
       " 'activation_date',\n",
       " 'user_type',\n",
       " 'image',\n",
       " 'image_top_1',\n",
       " 'deal_probability',\n",
       " 'has_description',\n",
       " 'has_price',\n",
       " 'has_params',\n",
       " 'has_image',\n",
       " 'month',\n",
       " 'day',\n",
       " 'weekday',\n",
       " 'user_ads_count',\n",
       " 'title_description_params',\n",
       " 'img_size',\n",
       " 'img_sharpness',\n",
       " 'img_luminance',\n",
       " 'img_colorfulness',\n",
       " 'img_confidence',\n",
       " 'img_keypoints',\n",
       " 'log_img_sharpness',\n",
       " 'log_img_keypoints',\n",
       " 'title_word_count',\n",
       " 'description_non_regular_chars_ratio',\n",
       " 'description_word_count',\n",
       " 'merged_params_word_count',\n",
       " 'description_sentence_count',\n",
       " 'description_words/sentence_ratio',\n",
       " 'title_capital_letters_ratio',\n",
       " 'description_capital_letters_ratio',\n",
       " 'title_non_regular_chars_ratio',\n",
       " 'title_num_of_newrow_char',\n",
       " 'description_num_of_newrow_char',\n",
       " 'title_num_adj',\n",
       " 'title_num_nouns',\n",
       " 'title_adj_to_len_ratio',\n",
       " 'title_noun_to_len_ratio',\n",
       " 'description_num_adj',\n",
       " 'description_num_nouns',\n",
       " 'description_adj_to_len_ratio',\n",
       " 'description_noun_to_len_ratio',\n",
       " 'title_first_noun_stemmed',\n",
       " 'title_second_noun_stemmed',\n",
       " 'title_third_noun_stemmed',\n",
       " 'description_first_noun_stemmed',\n",
       " 'description_second_noun_stemmed',\n",
       " 'description_third_noun_stemmed',\n",
       " 'title_first_adj_stemmed',\n",
       " 'title_second_adj_stemmed',\n",
       " 'title_third_adj_stemmed',\n",
       " 'description_first_adj_stemmed',\n",
       " 'description_second_adj_stemmed',\n",
       " 'description_third_adj_stemmed',\n",
       " 'title_sentiment',\n",
       " 'description_sentiment',\n",
       " 'avg_days_up_user',\n",
       " 'avg_times_up_user',\n",
       " 'n_user_items',\n",
       " 'log_item_seq_number',\n",
       " 'log_price',\n",
       " 'log_description_word_count',\n",
       " 'image_top_1_class',\n",
       " 'image_top_1_regression',\n",
       " 'image_top_1_rounded_regression',\n",
       " 'log_price_regression']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text towards input to an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_feature = 'title_description_params'\n",
    "\n",
    "train_y_prob = train['deal_probability']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "stopwords = set()\n",
    "with codecs.open(('stopwords_ru.txt'), encoding='cp1251') as ins:\n",
    "    for w in ins:\n",
    "        word = w.strip(\"\\r\\n\")\n",
    "        word = word.strip(\"\\n\")\n",
    "        stopwords.add(word.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize and Vectorize (keras encoded one-hot representation (each onehot vec represented as an int number)) text feature.\n",
    "\n",
    "See: https://keras.io/preprocessing/text/#one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing titles...\n",
      "Done tokenizing.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot, Tokenizer, text_to_word_sequence\n",
    "\n",
    "# Those consts are important for the NN itself\n",
    "max_words_in_desc_title_param = 150 # See text analysis notebook. 95% are shorter than a 110.\n",
    "word_embed_dim = 300\n",
    "\n",
    "train_x_title = train['title'].str.lower()\n",
    "test_x_title = test['title'].str.lower()\n",
    "tokenizer = Tokenizer(num_words = max_words_in_desc_title_param)\n",
    "all_titles = np.hstack([train_x_title, test_x_title])\n",
    "\n",
    "print('Tokenizing titles...')\n",
    "tokenizer.fit_on_texts(all_titles)\n",
    "print('Done tokenizing.')\n",
    "\n",
    "del all_titles\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default text_to_word_sequence automatically does 4 things:\n",
    "#   Splits words by space (split=” “), \n",
    "#   Filters out punctuation (filters=’!”#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n’).\n",
    "#   Converts text to lowercase (lower=True).\n",
    "# We add stopwords filtering to the process.\n",
    "\n",
    "def my_text_to_word_sequence(text):\n",
    "    result = []\n",
    "    for word in text_to_word_sequence(text):\n",
    "        if word not in stopwords:\n",
    "            result.append(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying tokenizer on titles...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "<lambda>() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-595b4ac3b2d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m train_x_title = train_x_title.apply(\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     [tokenizer.word_index[word] for word in my_text_to_word_sequence(r[text_feature])], axis=1)\n\u001b[0m\u001b[1;32m      6\u001b[0m test_x_title = test_x_title.apply(\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   3179\u001b[0m         \u001b[0;31m# handle ufuncs and lambdas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mufunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3181\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3182\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3183\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: <lambda>() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": [
    "print('Applying tokenizer on titles...')\n",
    "\n",
    "train_x_title = train_x_title.apply(\n",
    "    lambda r: \n",
    "    [tokenizer.word_index[word] for word in my_text_to_word_sequence(r[text_feature])], axis=1)\n",
    "test_x_title = test_x_title.apply(\n",
    "    lambda r: \n",
    "    [tokenizer.word_index[word] for word in my_text_to_word_sequence(r[text_feature])], axis=1)\n",
    "    \n",
    "train_x_title = pad_sequences(train_x_text[text_feature], maxlen=max_words_in_desc_title_param)\n",
    "test_x_title = pad_sequences(test_x_text[text_feature], maxlen=max_words_in_desc_title_param)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_description = train['description'].str.lower()\n",
    "test_x_description = test['description'].str.lower()\n",
    "tokenizer = Tokenizer(num_words = max_words_in_desc_title_param)\n",
    "all_descriptions = np.hstack([train_x_description, test_x_description])\n",
    "\n",
    "print('Tokenizing descriptions...')\n",
    "tokenizer.fit_on_texts(all_descriptions)\n",
    "print('Done tokenizing.')\n",
    "\n",
    "del all_descriptions\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Applying tokenizer on descriptions...')\n",
    "\n",
    "train_x_description = train_x_description.apply(\n",
    "    lambda r: \n",
    "    [tokenizer.word_index[word] for word in my_text_to_word_sequence(r[text_feature])], axis=1)\n",
    "test_x_description = test_x_description.apply(\n",
    "    lambda r: \n",
    "    [tokenizer.word_index[word] for word in my_text_to_word_sequence(r[text_feature])], axis=1)\n",
    "    \n",
    "train_x_description = pad_sequences(train_x_text[text_feature], maxlen=max_words_in_desc_title_param)\n",
    "test_x_description = pad_sequences(test_x_text[text_feature], maxlen=max_words_in_desc_title_param)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and load FastText (Facebook's) Russian wikipedia word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasttext_embedding_matrix(data_path, tokenizer, embedding_dim):\n",
    "    print('loading embeddings...')\n",
    "    \n",
    "    EMBEDDING_FILE_PATH = os.path.join(data_path, 'cc.ru.300.vec')\n",
    "    def get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE_PATH))\n",
    "\n",
    "    vocab_size = len(tokenizer.word_index) + 2\n",
    "    embedding_matrix = np.random.rand(vocab_size, embedding_dim)\n",
    "    \n",
    "    print('creating embedding matrix...')\n",
    "    embedding_exists = 0\n",
    "    no_embeddings = 0\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "            embedding_exists += 1\n",
    "        else:\n",
    "            no_embeddings += 1\n",
    "    \n",
    "    print (\"There are total of {} words in our corpus.\".format(embedding_exists+no_embeddings))\n",
    "    print (\"There are {} embeddings in FastText.\".format(len(embeddings_index)))\n",
    "    print (\"We have embeddings for {} words ({}% existing embeddings).\".format(embedding_exists, \\\n",
    "                                                                               (100*embedding_exists/(embedding_exists+no_embeddings))))\n",
    "    print (\"Embedding is missing for {} words.\".format(no_embeddings))\n",
    "\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    \n",
    "    print('done loading embeddings...')\n",
    "    return embedding_matrix, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix, vocab_size = get_fasttext_embedding_matrix(data_path=DATA_PATH, \\\n",
    "                                                             tokenizer=tokenizer, embedding_dim = word_embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning - Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_out_dim = 256\n",
    "out_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title LSTM:\n",
    "l_title_input = Input(shape=(max_words_in_desc_title_param,), name=\"title_lstm_input\")\n",
    "# Load FastText's weights:\n",
    "l_title_embedding = Embedding(input_dim=vocab_size, output_dim=word_embed_dim, weights = [embedding_matrix], \\\n",
    "                             input_length=max_words_in_desc_title_param, name='title_embeddings')(l_text_input)\n",
    "title_lstm_out = LSTM(lstm_out_dim, name='LSTM', dropout=0.2, recurrent_dropout=0.2)(l_title_embedding)\n",
    "\n",
    "# Description LSTM:\n",
    "l_description_input = Input(shape=(max_words_in_desc_title_param,), name=\"description_lstm_input\")\n",
    "# Load FastText's weights:\n",
    "l_description_embedding = Embedding(input_dim=vocab_size, output_dim=word_embed_dim, weights = [embedding_matrix], \\\n",
    "                             input_length=max_words_in_desc_title_param, name='description_embeddings')(l_text_input)\n",
    "description_lstm_out = LSTM(lstm_out_dim, name='LSTM', dropout=0.2, recurrent_dropout=0.2)(l_description_embedding)\n",
    "\n",
    "# Aggregate two LSTMs into the one final vector.\n",
    "l_aggregative = concatenate([title_lstm_out, description_lstm_out])\n",
    "\n",
    "# This is the main logistic regression output\n",
    "output = Dense(out_dim, activation='sigmoid', name='output')(l_aggregative)\n",
    "\n",
    "rmsprop_opt = RMSprop(lr=0.00001) # Best for training RNNs.\n",
    "def rmse_err(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "\n",
    "model = Model(l_text_input, output)\n",
    "model.compile(optimizer=rmsprop_opt, loss=[rmse_err]) # Higher weight for main output.\n",
    "print(model.summary())\n",
    "\n",
    "load = False\n",
    "fname = 'NN-JUST-LSTM-SEP-model-weights.h5'\n",
    "if load:\n",
    "    model.load_weights(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr_cd = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "save_weights_cd = ModelCheckpoint(fname, save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "nbatch_size = 512\n",
    "\n",
    "# We want to double batch size every two epochs.\n",
    "nepochs = 5\n",
    "for i in range(nepochs):\n",
    "    print ('Curr batch size: {}'.format(nbatch_size))\n",
    "    model.fit(train_x_text, train_y_prob, \n",
    "              validation_split = 0.1, \n",
    "              epochs=1, \n",
    "              batch_size=nbatch_size,\n",
    "              callbacks=[reduce_lr_cd, save_weights_cd])\n",
    "    nbatch_size *= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions to futher ensambels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict(train_x_text)\n",
    "y_pred_test = model.predict(test_x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train_df = pd.DataFrame(train['item_id'])\n",
    "res_train_df['deal_probability'] = y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_test_df = pd.DataFrame(test['item_id'])\n",
    "res_test_df['deal_probability'] = y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.concat([res_train_df, res_test_df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv('/home/u14303/Avito/Predictions/NN-JUST-LSTM-SEP.csv.gz', index=False, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel, 2018)",
   "language": "python",
   "name": "intel_distribution_of_python_3_2018"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
